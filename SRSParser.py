
#Imports#
import json
import os
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema.runnable import RunnableLambda
from tabulate import tabulate

# ‚úÖ Load environment variables
load_dotenv()

# ‚úÖ Define the prompt template
prompt_template = PromptTemplate(
    input_variables=["text"],
    template="""
    You are analyzing a Software Requirements Specification (SRS) document.

    Extract the following information:
    - **Topic**: Identify the main topic.
    - **Structure**: Structured, Unstructured, or One Statement.
    - **Complexity Level**: High or Low.

    Respond **ONLY** in **valid JSON format** without any extra text:
    {{
      "topic": "<Extracted Topic>",
      "structure": "<Structured/Unstructured/One Statement>",
      "level": "<High/Low>"
    }}

    Text: {text}
    """
)

# ‚úÖ Initialize OpenAI GPT with JSON enforced output
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
    model_kwargs={"response_format": {"type": "json_object"}}  # ‚úÖ Correct JSON enforcement
)

def parse_gpt_response(response):
    """Ensures GPT response is a valid JSON object."""
    try:
        raw_content = response.content if hasattr(response, "content") else response

        if isinstance(raw_content, dict):  # ‚úÖ Already JSON
            return raw_content
        elif isinstance(raw_content, str):  # ‚úÖ Parse JSON if string
            return json.loads(raw_content.strip())

        print(f"‚ùå Unexpected GPT response format: {type(raw_content)}")
        return None
    except json.JSONDecodeError:
        print(f"‚ùå JSON Parsing Error: Could not decode: {response}")
        return None


# ‚úÖ Create a runnable pipeline
chain = prompt_template | llm | RunnableLambda(parse_gpt_response)


def analyze_with_gpt(text):
    """Runs GPT analysis and ensures valid JSON output."""
    try:
        result = chain.invoke({"text": text[:8000]})  # ‚úÖ Limit text to 8000 chars for large docs
        return result if result else None
    except Exception as e:
        print(f"‚ùå GPT Processing Error: {e}")
        return None


def read_document(file_path):
    """Reads a text-based document (TXT, PDF, or DOCX) and returns its content."""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå File not found: {file_path}")

    ext = os.path.splitext(file_path)[1].lower()

    if ext == ".txt":
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read()

    elif ext == ".pdf":
        try:
            import PyPDF2  # ‚úÖ Ensure PyPDF2 is installed (`pip install PyPDF2`)
            with open(file_path, "rb") as file:
                reader = PyPDF2.PdfReader(file)
                return "\n".join(page.extract_text() for page in reader.pages if page.extract_text())

        except Exception as e:
            raise ValueError(f"‚ùå PDF parsing error: {e}")

    elif ext in [".doc", ".docx"]:
        try:
            import docx  # ‚úÖ Ensure `python-docx` is installed (`pip install python-docx`)
            doc = docx.Document(file_path)
            return "\n".join(para.text for para in doc.paragraphs)

        except Exception as e:
            raise ValueError(f"‚ùå DOCX parsing error: {e}")

    else:
        raise ValueError(f"‚ùå Unsupported file format: {ext}")


# ‚úÖ Process all files in a folder
def process_folder(folder_path):
    """Iterates through all files in the folder and processes them."""
    if not os.path.isdir(folder_path):
        raise ValueError(f"‚ùå The provided path is not a directory: {folder_path}")

    results = {}

    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)

        try:
            print(f"üîπ Processing file: {filename}")
            document_text = read_document(file_path)
            analysis_result = analyze_with_gpt(document_text)

            if analysis_result:
                results[filename] = analysis_result
            else:
                print(f"‚ùå Failed to analyze {filename}")

        except Exception as e:
            print(f"‚ùå Error processing {filename}: {e}")

    return results


# ‚úÖ Process folder and generate structured table output
if __name__ == "__main__":
    folder_path = r"C:\Users\itrad\Downloads\req"

    try:
        analysis_results = process_folder(folder_path)

        if analysis_results:
            # ‚úÖ Convert results to table format
            table_data = []
            for filename, data in analysis_results.items():
                table_data.append([
                    filename,
                    data.get("topic", "N/A"),
                    os.path.splitext(filename)[1],  # Extract file extension as format
                    "Unknown",  # Placeholder for Pages (you can extract it from word count)
                    "Unknown",  # Placeholder for Pictures/Tables (need image processing)
                    data.get("level", "N/A"),
                    data.get("structure", "N/A"),
                    "Unknown"  # Placeholder for Source
                ])

            # ‚úÖ Define column headers
            headers = [
                "Doc Name", "Topic", "Format", "Pages", "Pictures/Tables",
                "Level", "Structure", "Source"
            ]

            # ‚úÖ Print structured table output
            print("\n‚úÖ Final GPT Analysis Output:")
            print(tabulate(table_data, headers=headers, tablefmt="grid"))

        else:
            print("‚ùå No valid SRS documents processed.")

    except Exception as e:
        print(f"‚ùå Error: {e}")
